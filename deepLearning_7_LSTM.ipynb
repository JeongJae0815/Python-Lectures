{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import pandas\n",
    "import sys\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import csv\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import time\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level RNN\n",
    "\n",
    "Let's try to train something for next-letter prediction on Shakespeare.\n",
    "\n",
    "We will use a standard LSTM architecture with two LSTM-layers and one fully-connected layer at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how many epochs to use for training\n",
    "num_epochs = 10\n",
    "# how many layers the LSTM should have\n",
    "num_layers = 2\n",
    "# how many hidden units each LSTM cell should have\n",
    "lstm_size = 512\n",
    "# how many characters do we feed in at the same time\n",
    "# this is the time context\n",
    "seq_length = 25\n",
    "# how many of these sequences we put in as one batch\n",
    "batch_size = 50\n",
    "# the learning rate for gradient descent\n",
    "lr = 0.0002\n",
    "\n",
    "# how much of Shakespeare to use for training the model\n",
    "fraction_to_train = 0.5\n",
    "\n",
    "# reading Shakespeare as full text\n",
    "allData = open('data/shakespeare_plays.txt', 'r').read()\n",
    "allData = allData[:int(fraction_to_train*len(allData))].lower()\n",
    "# yet another way to read in the data\n",
    "# put them into a collections Counter container\n",
    "counter = collections.Counter(allData)\n",
    "# sort the items from highest to lowest [according to the second \n",
    "# element in counter.items()]\n",
    "count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "# the * operator unpacks the list, so that we only get the first column\n",
    "chars, _ = zip(*count_pairs)\n",
    "# count how many characters we have\n",
    "vocab_size = len(chars)\n",
    "# and then put them into a dictionary with indices\n",
    "vocab = dict(zip(chars, range(len(chars))))\n",
    "# this is our input \"tensor\", which is simply a way to map all the \n",
    "# characters in Shakespeare to our dictionary entries \n",
    "tensor = np.array(list(map(vocab.get, allData)))\n",
    "\n",
    "# this is the number of batches we process on our dataset\n",
    "num_batches = int(tensor.size / (batch_size * seq_length))\n",
    "\n",
    "# print the stats of our dataset\n",
    "print(\"Shakespeare has\",vocab_size,\"unique characters in a total of\",\n",
    "      tensor.size,\"characters. \\nFrom this, we make\",num_batches,\"batches.\")\n",
    "\n",
    "# take the subset that exactly matches the number of characters we can fit\n",
    "tensor = tensor[:num_batches * batch_size * seq_length]\n",
    "# this is the training data\n",
    "xdata = tensor\n",
    "# now make the targets - we first copy the training data [to get the size]\n",
    "ydata = np.copy(tensor)\n",
    "# then we shift everything by one\n",
    "ydata[:-1] = xdata[1:]\n",
    "# and the last element becomes the first element - i.e., we wrap the dataset!\n",
    "ydata[-1] = xdata[0]\n",
    "# this is the final data, but now batched\n",
    "x_batches = np.split(xdata.reshape(batch_size, -1),\n",
    "                     num_batches, 1)\n",
    "y_batches = np.split(ydata.reshape(batch_size, -1),\n",
    "                     num_batches, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The LSTM training\n",
    "\n",
    "In the following, we will build the tensorflow model for predicting characters.\n",
    "\n",
    "This is a sequence-to-sequence model. That is, our input consists of a sequence, and our targets also consists of a sequence!\n",
    "\n",
    "Note: In principle, this would be better as a function, or even better as a class, but for debugging purposes, I have exposed the functionality here inside the main memory space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# contains the layers of LSTM cells, each with a pre-defined hidden units\n",
    "model_cells = []\n",
    "for _ in range(num_layers):\n",
    "    model_cells.append(tf.contrib.rnn.BasicLSTMCell(lstm_size))\n",
    "\n",
    "# final LSTM structure\n",
    "model_cell=tf.contrib.rnn.MultiRNNCell(model_cells,state_is_tuple=True)\n",
    "\n",
    "# input data and labels - note the dimensionality!\n",
    "model_input_data = tf.placeholder(tf.int32,[batch_size,seq_length])\n",
    "model_targets = tf.placeholder(tf.int32,[batch_size,seq_length])\n",
    "\n",
    "# we start with a clean slate for all initial states in the batch\n",
    "model_init_state = model_cell.zero_state(batch_size,tf.float32)\n",
    "\n",
    "# now we initialize all variables\n",
    "\n",
    "# these are the usual weights and biases - again, note the dimensions\n",
    "model_softmax_w = tf.Variable(tf.random_normal([lstm_size, vocab_size]))\n",
    "model_softmax_b = tf.Variable(tf.random_normal([vocab_size]))\n",
    "\n",
    "# this is the character embedding, which will be used to determine the\n",
    "# data structure - it is necessary for setting up the next line\n",
    "model_embedding = tf.Variable(tf.random_normal([vocab_size,lstm_size]))\n",
    "\n",
    "# this function is tricky:\n",
    "# in principle, our data input would otherwise consist of a one-hot\n",
    "# encoded vector (since we are doing classification)\n",
    "# one-hot encoding, however, is simply like a matrix look-up operation,\n",
    "# so this function does that\n",
    "# for the model_input_data (which consists of numbers between 0 and 57),\n",
    "# it retrieves the corresponding rows of model_embedding\n",
    "model_inputs = tf.nn.embedding_lookup(model_embedding, model_input_data)\n",
    "\n",
    "# this input will need to be split into seq_length chunks \n",
    "# (along the second axis!)\n",
    "model_inputs = tf.split(model_inputs,seq_length,1)\n",
    "\n",
    "# and we concatenate all elements again into a longer \"vector\"\n",
    "model_inputs = [tf.squeeze(x_,[1]) for x_ in model_inputs]\n",
    "\n",
    "# this uses a sequence-to-sequence learning scheme, which\n",
    "# unrolls the RNN back in time - all of our reformatting before\n",
    "# was necessary, so that we can fit the data into this function!\n",
    "model_outputs, model_last = tf.contrib.legacy_seq2seq.rnn_decoder(\n",
    "    model_inputs,model_init_state,model_cell,loop_function=None)\n",
    "\n",
    "# we get the output, and now will need to transform it back \n",
    "# to the original shape\n",
    "model_output = tf.reshape(tf.concat(model_outputs,1),[-1,lstm_size])\n",
    "\n",
    "# on this output, we perform a final, fully connected layer pass\n",
    "model_logits = tf.matmul(model_output,model_softmax_w)+model_softmax_b\n",
    "\n",
    "# and convert the output of that layer to probabilities\n",
    "model_probs = tf.nn.softmax(model_logits)\n",
    "\n",
    "# this loss calculates the cross-entropy loss for a sequence of probabilities\n",
    "# this will be a vector of [batch_size*seq_length] values\n",
    "model_loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "    [model_logits],  # probabilities input [batch_size*seq_length x vocab_size]\n",
    "    [tf.reshape(model_targets,[-1])],  # targets [batch_size*seq_length]\n",
    "    [tf.ones([batch_size*seq_length])]) # weights for each example (set to one here)\n",
    "\n",
    "# total \"cost\" or loss is simply the sum over all \n",
    "# [batch_size*seq_length] values - note, that we always have to deal\n",
    "# with a full batch in learning here!\n",
    "model_cost = tf.reduce_sum(model_loss)/batch_size/seq_length\n",
    "\n",
    "# and we save the last state of the LSTM for later\n",
    "model_final_state = model_last\n",
    "\n",
    "# this is the learning rate - it may be variable, but should NOT\n",
    "# be updated by the optimizer!\n",
    "model_lr = tf.Variable(lr,trainable = False)\n",
    "\n",
    "# these variables should be updated, however\n",
    "# they are:\n",
    "# --- model_softmax_w\n",
    "# --- model_softmax_b\n",
    "# --- model_embedding\n",
    "# --- the internal weights of the two LSTM cells\n",
    "model_tvars = tf.trainable_variables()\n",
    "\n",
    "# evaluate the gradients of all trainable variables with respect to the loss\n",
    "model_grads = tf.gradients(model_cost,model_tvars)\n",
    "\n",
    "# initialize the optimization scheme (ADAM is very efficient)\n",
    "model_optimizer = tf.train.AdamOptimizer(model_lr)\n",
    "\n",
    "# define an operation for one training step using the gradients and variables\n",
    "model_train_operation = model_optimizer.apply_gradients(\n",
    "    zip(model_grads,model_tvars))\n",
    "###################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "Now we code the actual training loop. We have to go through epochs of training and then in each epoch run each batch through a forward pass, gradient pass and weight update. \n",
    "\n",
    "Since tensorflow evaluates the computational graph, when we invoke the names of the tensors associated with the outcome, we simply call `run` on  the loss, final state, and a train operation each time. \n",
    "\n",
    "Importantly, our LSTM has \"memory\", so we keep the state and feed it into the network at the next time step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# starting the tensorflow interactive version\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# run a number of epochs\n",
    "for e in range(num_epochs):\n",
    "    # in each epoch we start \"fresh\"\n",
    "    state=sess.run(model_init_state)\n",
    "    batch = 0\n",
    "    # loop through all batches we have\n",
    "    for b in range(num_batches):\n",
    "        start = time.time()\n",
    "        # get the current batch consisting of input characters \n",
    "        # and target characters\n",
    "        x,y = x_batches[batch],y_batches[batch]\n",
    "        # present this to the model via a dictionary\n",
    "        feed = {model_input_data: x, model_targets: y}\n",
    "        # go through the initial states (2 in our case for each cell)\n",
    "        for i, (c, h) in enumerate(model_init_state):\n",
    "            # and initialize the coefficients of the LSTM from the\n",
    "            # previous state\n",
    "            feed[c] = state[i].c\n",
    "            feed[h] = state[i].h\n",
    "        # get the training loss on the batch and the new state\n",
    "        train_loss, state, _ = sess.run([model_cost, model_final_state, model_train_operation], feed)    \n",
    "        end=time.time()\n",
    "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"\n",
    "                  .format(e * num_batches + b,\n",
    "                          num_epochs * num_batches,\n",
    "                          e, train_loss, end - start))\n",
    "        batch+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the output of tensorflow\n",
    "Since we trained a bit, let's try to save the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(None)\n",
    "saver.save(sess, 'char_LSTM_small2epochs.ckpt', global_step=e * num_batches + b)\n",
    "print([str(i.name) for i in tf.global_variables()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from the model\n",
    "\n",
    "Now for the next tricky bit. Given the trained model, we would like to sample from it.\n",
    "\n",
    "For this, we need need to re-use the existing LSTM-architecture (i.e., the weights of course), but we need to change one thing:\n",
    "\n",
    "The batch_size and the seq_length variables need to be changed to \"1\". We now want to predict from 1 character already, and we do not do any batches here.\n",
    "\n",
    "In the tensorflow tutorial, there is another change to the sampling version of the model:\n",
    "\n",
    "They add another evaluation-function on top that will be used in the decoding step in order to decide what to put into the next LSTM decoding step given the current state. That is, instead of inputting the full output, the LSTM's parameters are used to sample from its output distribution. This is done by introducing a `loop_function` parameter for the `seq2seq` helper function. In this step one could also introduce attentional parameters, for example, that help the LSTM to decide what to pay attention to.\n",
    "\n",
    "See \n",
    "https://theneuralperspective.com/2016/11/20/recurrent-neural-network-rnn-part-4-attentional-interfaces/\n",
    "for more information on how to implement attentional interfaces with the tensorflow architecture!\n",
    "\n",
    "For our model, this step does not make a whole lot of difference, so I have put in the function definition, but not activated it for the sampling stage.\n",
    "\n",
    "So, let's define our LSTM for this case again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_length = 1\n",
    "\n",
    "# contains the number of LSTM cells\n",
    "model_cells = []\n",
    "for _ in range(num_layers):\n",
    "    model_cells.append(tf.contrib.rnn.BasicLSTMCell(lstm_size))\n",
    "\n",
    "# final LSTM structure\n",
    "model_cell=tf.contrib.rnn.MultiRNNCell(model_cells,state_is_tuple=True)\n",
    "\n",
    "\n",
    "# input data and labels\n",
    "model_input_data = tf.placeholder(tf.int32,[batch_size,seq_length])\n",
    "model_targets = tf.placeholder(tf.int32,[batch_size,seq_length])\n",
    "\n",
    "# we start with a clean slate for all initial states in the batch\n",
    "model_init_state = model_cell.zero_state(batch_size,tf.float32)\n",
    "\n",
    "# now we initialize all variables\n",
    "\n",
    "# these are the usual weights and biases\n",
    "model_softmax_w = tf.Variable(tf.random_normal([lstm_size, vocab_size]))\n",
    "model_softmax_b = tf.Variable(tf.random_normal([vocab_size]))\n",
    "\n",
    "# this is the character embedding, which will be used to determine the\n",
    "# data structure\n",
    "model_embedding = tf.Variable(tf.random_normal([vocab_size,lstm_size]))\n",
    "\n",
    "# this function is tricky:\n",
    "# for the model_input_data (which consists of numbers between 0 and 58),\n",
    "# it retrieves the corresponding rows of model_embedding\n",
    "model_inputs = tf.nn.embedding_lookup(model_embedding, model_input_data)\n",
    "\n",
    "# this input will need to be split into seq_length chunks \n",
    "# (along the first axis)\n",
    "model_inputs = tf.split(model_inputs,seq_length,1)\n",
    "\n",
    "# and we concatenate all elements again into a longer \"vector\"\n",
    "model_inputs = [tf.squeeze(x_,[1]) for x_ in model_inputs]\n",
    "\n",
    "# this loop function can be used to select features in each LSTM step according\n",
    "# to the existing distribution and then to feed a weighted version of these\n",
    "# into the next step of the LSTM\n",
    "def loop(prev,_):\n",
    "    prev = tf.matmul(prev,model_softmax_w)+model_softmax_b\n",
    "    prev_symbol = tf.stop_gradient(tf.argmax(prev,1))\n",
    "    return tf.nn.embedding_lookup(model_embedding,prev_symbol)\n",
    "\n",
    "\n",
    "# this uses a sequence two sequence learning scheme, which\n",
    "# unrolls the RNN back in time\n",
    "\n",
    "# note, we set loop_function=None here, if you set\n",
    "# loop_function=loop, it will use the above sampling function\n",
    "# to reweight data that goes into the next stage of the LSTM processing\n",
    "# for our architecture this does not make a big difference, but for\n",
    "# deeper networks with more LSTM layers it may!\n",
    "model_outputs, model_last = tf.contrib.legacy_seq2seq.rnn_decoder(\n",
    "    model_inputs,model_init_state,model_cell,loop_function=loop)\n",
    "\n",
    "# the output will need to be transformed back to the original shape\n",
    "model_output = tf.reshape(tf.concat(model_outputs,1),[-1,lstm_size])\n",
    "\n",
    "# now we perform our standard weight operation\n",
    "model_logits = tf.matmul(model_output,model_softmax_w)+model_softmax_b\n",
    "\n",
    "# and convert to probabilities\n",
    "model_probs = tf.nn.softmax(model_logits)\n",
    "\n",
    "# this loss calculates the cross-entropy loss for a sequence of probabilities\n",
    "# this will be a vector of [batch_size*seq_length] values\n",
    "model_loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "    [model_logits],  # probabilities input [batch_size*seq_length x vocab_size]\n",
    "    [tf.reshape(model_targets,[-1])],  # targets [batch_size*seq_length]\n",
    "    [tf.ones([batch_size*seq_length])]) # weights for each example (set to one here)\n",
    "\n",
    "# total \"cost\" or loss is simply the sum over all [batch_size*seq_length] values\n",
    "model_cost = tf.reduce_sum(model_loss)/batch_size/seq_length\n",
    "\n",
    "# and we save the last state of the LSTM\n",
    "model_final_state = model_last\n",
    "\n",
    "# this is the learning rate - it may be variable, but should NOT\n",
    "# be updated by the optimizer!\n",
    "model_lr = tf.Variable(lr,trainable = False)\n",
    "\n",
    "# these variables should be updated, however\n",
    "# they are:\n",
    "# --- model_softmax_w\n",
    "# --- model_softmax_b\n",
    "# --- model_embedding\n",
    "# --- the internal weights of the two LSTM cells\n",
    "model_tvars = tf.trainable_variables()\n",
    "\n",
    "# evaluate the gradients of all trainable variables with respect to the loss\n",
    "model_grads = tf.gradients(model_cost,model_tvars)\n",
    "\n",
    "# initialize the optimization scheme (ADAM is very efficient)\n",
    "model_optimizer = tf.train.AdamOptimizer(model_lr)\n",
    "\n",
    "# define an operation for one training step using the gradients and variables\n",
    "model_train_operation = model_optimizer.apply_gradients(zip(model_grads,model_tvars))\n",
    "###################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the model back from disk. For this, we initialize all variables and then load the model back from the last `checkpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now let's go\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(None)\n",
    "ckpt = tf.train.get_checkpoint_state('.')\n",
    "saver.restore(sess,ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the actual sampling. \n",
    "\n",
    "For this, we first initialize the model cells to zero and then push the \"prime\", that is, the start of the sentence into the model.\n",
    "\n",
    "Here, we have selected \"the \" as the prime, so the sentence will start with these four characters. We simply convert these into our usual vocabulary vector and push it into the model. The output will be the state after \"the \" has been seen.\n",
    "\n",
    "Now for the next characters, we feed the last character into the model, get the new state and the probabilities for all 58 characters. Now, we could simply do an `argmax` on this, but this would most likely get us into infinite loops again. So, we do a weighted choice, in which we take our probabilities, sum them all up, and then get the index from that array in which a random number from 0-1 would be inserted via the `np.searchsorted` function.\n",
    "\n",
    "This index is converted into a character, added to our return string and then the process is repeated with the current state of the LSTM being updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "state = sess.run(model_cell.zero_state(1, tf.float32))\n",
    "prime = 'the '\n",
    "for char in prime:\n",
    "    x = np.zeros((1, 1))\n",
    "    x[0, 0] = vocab[char]\n",
    "    feed = {model_input_data: x, model_init_state: state}\n",
    "    [state] = sess.run([model_final_state], feed)\n",
    "\n",
    "def weighted_pick(weights):\n",
    "    t = np.cumsum(weights)\n",
    "    s = np.sum(weights)\n",
    "    return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "    \n",
    "ret = prime\n",
    "char = prime[-1]\n",
    "for n in range(500):\n",
    "    x = np.zeros((1, 1))\n",
    "    x[0, 0] = vocab[char]\n",
    "    feed = {model_input_data: x, model_init_state: state}\n",
    "    [probs, state] = sess.run([model_probs, model_final_state], feed)\n",
    "    p = probs[0]\n",
    "    sample = weighted_pick(p)\n",
    "    pred = chars[sample]\n",
    "    ret += pred\n",
    "    char = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
