{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import pandas\n",
    "import sys\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import csv\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction into tensorflow\n",
    "\n",
    "Let's extend our Python journey into deep networks to one of the standard neural network packages.\n",
    "\n",
    "First, installation:\n",
    "\n",
    "## Installing tensorflow\n",
    "\n",
    "You can follow the many examples given here:\n",
    "\n",
    "https://www.tensorflow.org/install/\n",
    "\n",
    "To reduce the hassle, I would recommend a **CPU-installation**, since GPU-installs are notoriously difficult and may result in you spending several hours tweaking drivers and installation files. \n",
    "\n",
    "I actually had good success with installing the pip-version on my computer, so I did:\n",
    "\n",
    "`pip3 install tensorflow` \n",
    "\n",
    "and it just rolled - after downloading for several minutes on a high-speed network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow basics: the computational graph\n",
    "\n",
    "In tensorflow, everything is structured as a computational graph, which is just a fancy word for a flow-chart.\n",
    "\n",
    "A computational graph is a series of operations arranged into a graph with nodes. You can visualize this graph as a flow-chart with tools shipped in tensorflow as well, which makes for nice debugging and a different intuition about the computations that are going on in your code.\n",
    "\n",
    "### tensors\n",
    "\n",
    "As the name says, tensorflow works on tensors, which are mathematical entities that basically generalize matrices. In Python terms that means roughly that they are like multi-dimensional arrays - a matrix has two indices `m[i][j]`. This matrix is actually simply a tensor of Rank 2.\n",
    "\n",
    "Hence, a tensor of Rank 3 would be `t[i][j][k]`. Things get a little tricky with summations and multiplications of tensors, but in principle tensors are basically multi-dimensionally-indexed \"matrices\".\n",
    "\n",
    "### nodes\n",
    "\n",
    "Each node in tensorflow takes zero or more tensors as input, and produces a tensor as an output.\n",
    "\n",
    "The most basic node is a \"constant\" node that takes zero inputs (since it is constant) and produces a Rank 1 tensor as output (a number).\n",
    "\n",
    "Let's create two of these very simple nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodeBoring1 = tf.constant(5.0)\n",
    "nodeBoring2 = tf.constant(10.0)\n",
    "print(nodeBoring1,nodeBoring2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important thing is that printing the nodes does not print their values. Instead a node is a structure in a computational graph that needs to be evaluated in order to produce output!\n",
    "\n",
    "We evalulate the computational graph by running a session, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([nodeBoring1,nodeBoring2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple computations with nodes\n",
    "\n",
    "Let's multiply two nodes together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodeMult = tf.multiply(nodeBoring1,nodeBoring2)\n",
    "print(nodeMult)\n",
    "print(sess.run(nodeMult))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ain't that awesome? We can use a multi-megabyte code-based to multiply two numbers in about 4 lines of code...\n",
    "\n",
    "Ok, sarcasm off. Let's try to visualize the computational graph. Since we are using jupyter, we first have to teach it how to display the input from tensorboard, which is tensorflow's official visualization tool for these graphs.\n",
    "\n",
    "The following code should allow us to use its visualization in jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome again! We have two constant input nodes, and apparently they are combined in a multiply node! <\"Totally faints\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding variable nodes\n",
    "\n",
    "Ok, enough with the constants. Let's use variable nodes in our graph, so that we can go forward with something interesting.\n",
    "\n",
    "Let's try to do a simple linear regression.\n",
    "\n",
    "For this, we need: \n",
    "\n",
    "* two variables $w$ and $b$ that hold the slope and the intercept of the line\n",
    "\n",
    "* a placeholder $x$ that will hold our input data\n",
    "\n",
    "* a model $ym=wx+b$ that combines everything together\n",
    "\n",
    "* a loss function $l=\\sum(y-ym)^2$ that evaluates how the predictions fit the actual data $y$\n",
    "\n",
    "Here's the full code in tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# components of the model\n",
    "w = tf.Variable([.5])\n",
    "b = tf.Variable([-.5])\n",
    "\n",
    "# input data - we need to tell tensorflow the datatype!\n",
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "# actual data\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# linear model that produces predictions\n",
    "ym = w*x + b\n",
    "\n",
    "##### IMPORTANT\n",
    "# we need to initialize variables before use!!!\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "#####\n",
    "\n",
    "# let's see the output of the model with some input data\n",
    "print(\"linear predictions\",sess.run(ym, {x:[0,1,2,3,4,5]}))\n",
    "\n",
    "# now how good are we?\n",
    "l = tf.reduce_sum(tf.square(ym-y))\n",
    "print(\"loss =\",sess.run(l, {x:[0,1,2,3,4,5], y:[0,0.3,0.6,0.9,1.2,1.5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the predictions are off, since the model parameters are, of course, not ideal. So let's change them using the `tf.assign` method, which changes already-initialized variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optw = tf.assign(w,[0.3])\n",
    "optb = tf.assign(b,[0])\n",
    "sess.run([optw,optb])\n",
    "print(\"loss =\",sess.run(l, {x:[0,1,2,3,4,5], y:[0,0.3,0.6,0.9,1.2,1.5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow basics: training something\n",
    "\n",
    "Of course, we would like to get $w,b$ automatically from the input data and the actual data! For this, we need to use some sort of optimization scheme in tensorflow.\n",
    "\n",
    "The most general (and simple) optimization scheme is gradient descent, so let's use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose the optimizer and the learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "# determine the loss function to optimize\n",
    "train = optimizer.minimize(l)\n",
    "# this will return our variables to the initial state!!\n",
    "sess.run(init)\n",
    "\n",
    "for i in np.arange(1000):\n",
    "    sess.run(train,{x:[0,1,2,3,4,5], y:[0,0.3,0.6,0.9,1.2,1.5]})\n",
    "\n",
    "print(\"final parameters:\",sess.run([w,b]))\n",
    "print(\"final loss:\",sess.run(l,{x:[0,1,2,3,4,5], y:[0,0.3,0.6,0.9,1.2,1.5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better - after a few iterations, the optimizer has successfully converged and we get our optimal solutions.\n",
    "\n",
    "The computational graph of our problem so far, however, now looks vastly more complicated due to the inclusion of the gradient descent optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
