{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.datasets import load_iris,fetch_mldata\n",
    "import time\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Clustering refers to unsupervised learning methods that take data points and assign them to $k$ different clusters.\n",
    "\n",
    "There are many different kinds of clustering algorithms, for a classical review look at:\n",
    "\n",
    "Jain, Anil K., M. Narasimha Murty, and Patrick J. Flynn. \"Data clustering: a review.\" ACM Computing Surveys (CSUR) 31.3 (1999): 264-323.\n",
    "\n",
    "For a more recent review investigating properties of clustering algorithms for *big data* look at:\n",
    "\n",
    "Fahad, Adil, et al. \"A survey of clustering algorithms for big data: Taxonomy and empirical analysis.\" IEEE Transactions on Emerging Topics in Computing 2.3 (2014): 267-279.\n",
    "\n",
    "## Desirable characteristics of clustering algorithms\n",
    "\n",
    "Here are some desirable characteristics of clustering algorithms (CA):\n",
    "\n",
    "* *robustness to noise and outliers*: most real-world data contains noisy points and/or outliers that may influence the clustering. It will be desirable for a CA to be conservative in its clustering, that is, to only assign those points to cluster that actually belong there. This property - while intuitive - is actually hard to get, since that means that CAs do NOT assign cluster labels to EVERY point!\n",
    "\n",
    "* *Occam's razor*: we can fit pretty much any distribution with enough parameters. In order to avoid over-fitting and in order for the researcher to properly control the CA, the number of its parameters should be small *and* they should be intuitively understandable so that they can be changed upon inspection of the clustering result\n",
    "\n",
    "* *stability*: many algorithms start with random initializations. A stable CA is one that returns the same clustering result for different initializations.\n",
    "\n",
    "* *performance with high-dimensional data*: a good CA can handle high-dimensional feature spaces\n",
    "\n",
    "* *performance scalability*: a good CA scales from small-scale to large-scale problems.\n",
    "\n",
    "## Taxonomy\n",
    "\n",
    "![title](images/clusteringAlgorithmsTaxonomy.png)\n",
    "\n",
    "\n",
    "## Some performance characteristics\n",
    "\n",
    "![title](images/clusteringAlgorithmsOverview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test some clustering algorithms on two datasets - a 2D dataset that was designed to showcase some difficulties in clustering and our standard IRIS dataset, for which we actually know the ground truth (that is, we have 3 clusters of flowers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load 2D data from the authors of HDBSCAN\n",
    "data2D = np.load('data/clusterable_data.npy')\n",
    "# load the good old IRIS dataset\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the 2D dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.set_context('notebook') # set to 'poster' for large-scale, high-quality output\n",
    "sb.set_color_codes() # set standard colors from seaborn\n",
    "plot_args = {'alpha' : 0.25, 's' : 80, 'linewidths':0} # plot data with slightly transparent look\n",
    "plt.scatter(data2D.T[0],data2D.T[1],c='b',**plot_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define clustering functions\n",
    "\n",
    "The following codes define helper functions that are designed to run a clustering using the `sklearn` interface and the to plot its result using `seaborn` functions. In addition, for the IRIS dataset, the function also counts the number of points for each of the 3 ground-truth flower classes and plots the ground-truth into the clustering (for the first 2 of the 4 dimensions in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate clustering algorithm on data\n",
    "def plot_clusters2D(data, algorithm, args, kwds):\n",
    "    # time the algorithm\n",
    "    start_time = time.time()\n",
    "    # use the algorithm based on the sklearn API with arguments and keywords supplied\n",
    "    labels = algorithm(*args, **kwds).fit_predict(data)\n",
    "    # time\n",
    "    end_time = time.time()\n",
    "    # now plot using a good color palette from seaborn initialized with the\n",
    "    # number of labels found by the clustering algorithm  \n",
    "    palette = sb.color_palette('deep', np.unique(labels).max() + 1)\n",
    "    # assign the colors to points\n",
    "    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
    "    f, ax = plt.subplots(figsize=(6,6))\n",
    "    # now scatter the data with standard parameters\n",
    "    ax.scatter(data.T[0], data.T[1], c=colors, **plot_args)\n",
    "    # get rid of axes\n",
    "    frame = f.gca()\n",
    "    frame.axes.get_xaxis().set_visible(False)\n",
    "    frame.axes.get_yaxis().set_visible(False)\n",
    "    # add descriptive title and print out time\n",
    "    plt.suptitle('Clusters found by {} took {:.2f} s'.format(str(algorithm.__name__),end_time - start_time), fontsize=14)\n",
    "    \n",
    "    # evaluate clustering algorithm on data\n",
    "def plot_clustersIRIS(iris_data, algorithm, args, kwds):\n",
    "    # time the algorithm\n",
    "    start_time = time.time()\n",
    "    # use the algorithm based on the sklearn API with arguments and keywords supplied\n",
    "    labels = algorithm(*args, **kwds).fit_predict(iris_data.data)\n",
    "    # time\n",
    "    end_time = time.time()\n",
    "    # here we simply print out how many cluster labels we have for each of the three\n",
    "    # ground-truth IRIS flower classes\n",
    "    for label in np.unique(labels):\n",
    "        c1 = len(np.where(labels[:50]==label)[0])\n",
    "        c2 = len(np.where(labels[50:100]==label)[0])\n",
    "        c3 = len(np.where(labels[100:]==label)[0])\n",
    "        print(label,c1,c2,c3)\n",
    " \n",
    "    # now plot using a good color palette from seaborn initialized with the\n",
    "    # number of labels found by the clustering algorithm  \n",
    "    palette = sb.color_palette('deep', np.unique(labels).max() + 1)\n",
    "    # assign the colors to points\n",
    "    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
    "    f, ax = plt.subplots(figsize=(6,6))\n",
    "    # now scatter the data with standard parameters\n",
    "    ax.scatter(iris_data.data.T[0], iris_data.data.T[1], c=colors, **plot_args)\n",
    "    for idx in range(len(iris_data.data)):\n",
    "        plt.text(iris_data.data.T[0][idx], iris_data.data.T[1][idx], str(iris_data.target[idx]))\n",
    "    # get rid of axes\n",
    "    frame = f.gca()\n",
    "    frame.axes.get_xaxis().set_visible(False)\n",
    "    frame.axes.get_yaxis().set_visible(False)\n",
    "    # add descriptive title and print out time\n",
    "    plt.suptitle('{} found {} clusters, took {:.2f}s'.format(str(algorithm.__name__),len(np.unique(labels)),end_time - start_time), fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means\n",
    "\n",
    "K-means is a very simple, iterative partitioning algorithm, which takes all datapoints and assigns them into partitions in the feature space using the centroids of the partitions.\n",
    "\n",
    "```\n",
    "select k random points as centroids\n",
    "\n",
    "repeat\n",
    "    assign each point to the nearest cluster centroid\n",
    "    update centroids\n",
    "until (centroid location does not change)\n",
    "```\n",
    "\n",
    "* *robustness to noise and outliers*: not good\n",
    "\n",
    "* *Occam's razor*: has one parameter, which is the number of clusters - a rather simple, but also very \"knowledgable\" parameter.\n",
    "\n",
    "* *stability*: can converge to different solutions depending on the initialization - best to run multiple times\n",
    "\n",
    "* *performance with high-dimensional data*: excellent\n",
    "\n",
    "* *performance scalability*: usually good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_clusters2D(data2D, cluster.KMeans, (), {'n_clusters':6})\n",
    "plot_clustersIRIS(iris, cluster.KMeans, (), {'n_clusters':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean-shift\n",
    "\n",
    "The underlying idea is that there is some probability density function from which our datapoints are drawn. Mean Shift then places centroids of clusters at the maxima of that density function in feature space. We get the density functions from kernel density estimation techniques https://en.wikipedia.org/wiki/Kernel_density_estimation. Therefore, the most important parameter of mean shift is the **bandwidth of the kernel**.  \n",
    "\n",
    "* *robustness to noise and outliers*: ok\n",
    "\n",
    "* *Occam's razor*: bandwidth of the kernel may be easier to guess than the number of clusters - you can get a feeling for it by looking at the distance matrix of the pairwise distances of points, for example.\n",
    "\n",
    "* *stability*: usually robust\n",
    "\n",
    "* *performance with high-dimensional data*: slow\n",
    "\n",
    "* *performance scalability*: good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_clusters2D(data2D, cluster.MeanShift, (0.175,), {'cluster_all':False})\n",
    "plot_clustersIRIS(iris, cluster.MeanShift, (2,), {'cluster_all':False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral clustering\n",
    "\n",
    "Spectral clustering is a two-stage process, in which in the first stage a graph will be constructed based on the feature distances between datapoints. Spectral clustering will use some mathematical property of this graph (in particular, its Laplacian) to find a (potentially lower-dimensional) embedding of the graph into Euclidean space. As we can see from this description, this algorithm combines manifold learning with clustering - actually, the main step is the embedding, which in most cases is followed by standard k-means in the transformed space.\n",
    "\n",
    "* *robustness to noise and outliers*: slightly better than k-means, but this is still at heart a partitional approach if k-means is used as the second stage\n",
    "\n",
    "* *Occam's razor*: same as k-means\n",
    "\n",
    "* *stability*: a bit more robust than k-means, since the embedding step can take care of the some initialization differences\n",
    "\n",
    "* *performance with high-dimensional data*: not so good\n",
    "\n",
    "* *performance scalability*: ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_clusters2D(data2D, cluster.SpectralClustering, (), {'n_clusters':6})\n",
    "plot_clustersIRIS(iris, cluster.SpectralClustering, (), {'n_clusters':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative clustering - hierarchical clustering\n",
    "\n",
    "This is a family of techniques in which clusters are created by iteratively assigning points to clusters based on some kind of pair-wise distance matrix. The distance matrix is updated in each step - this means that we need to have some kind of measure of how to measure  **distances between clusters**. There are many ideas how to do this (take the average of all distances between clusters, or their minimum, or a weighted distance, etc.).\n",
    "\n",
    "One good element of this technique that it is possible to visualize the clustering using a hierarchy from the sequence of merging points together. \n",
    "\n",
    "![title](images/tree_of_life.jpg)\n",
    "\n",
    "* *robustness to noise and outliers*: not so good, but can potentially be alleviated by stopping early\n",
    "\n",
    "* *Occam's razor*: if you want clusters, you need to pick when to stop the merging process, so that's the same as k-means\n",
    "\n",
    "* *stability*: good\n",
    "\n",
    "* *performance with high-dimensional data*: good\n",
    "\n",
    "* *performance scalability*: good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_clusters2D(data2D, cluster.AgglomerativeClustering, (), {'n_clusters':6, 'linkage':'ward'})\n",
    "plot_clustersIRIS(iris, cluster.AgglomerativeClustering, (), {'n_clusters':3, 'linkage':'ward'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "\n",
    "DBSCAN tries to find dense clusters - hence it is **not** a partitioning algorithm like many of the ones we looked at above. It first tries to transform the feature space by leaving points in dense regions alone, whereas points in sparse regions are moved further away in its distance metric. We then apply standard agglomerative clustering to the transformed space, which gives us a dendrogram that we then cut at a certain similarity threshold to get the clusters (this parameter is called *epsilon*). \n",
    "\n",
    "When we cut the dendrogram, we necessarily get some clusters that only contain a single point. These clusters are discarded as **noise clusters**.  \n",
    "\n",
    "In the following, if we set *epsilon=100*, all points will be noise-clusters. If we set *epsilon=200*, we get one cluster of points (2,5), and we set *epsilon=220*, we get two clusters (2,5) and (3,4)\n",
    "\n",
    "![title](images/dendrogram.png)\n",
    "\n",
    "* *robustness to noise and outliers*: good\n",
    "\n",
    "* *Occam's razor*: epsilon can be hard to pick, but it is sensitive to the exact choice, the density-based transformation depends on min_samples in the `sklearn` implementation as well\n",
    "\n",
    "* *stability*: stable across runs, but **not stable** across parameters!\n",
    "\n",
    "* *performance with high-dimensional data*: good! this algorithm lends itself to various performance tricks because of the local processing that is going on. \n",
    "\n",
    "* *performance scalability*: good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_clusters2D(data2D, cluster.DBSCAN, (), {'eps':0.025})\n",
    "# played with parameters for IRIS\n",
    "plot_clustersIRIS(iris, cluster.DBSCAN, (), {'eps':0.8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN\n",
    "\n",
    "This is an improved version of DBSCAN. First the algorithm does a density transformation, followed by agglomerative clustering in the transformed space. For treating the dendrogram, it takes a different approach: the dendrogram is condensed by viewing splits that result in a small number of points splitting off as points that will easily leave a cluster and hence may be noise points. From this we get a smaller dendrogram that potentially has fewer clusters that contain such noise points. Overall, the process allows the dendrogram to be cut at **different heights**. \n",
    "\n",
    "The parameter of HDBSCAN is now min_cluster_size, which is used in the final step to determine the minimum number of points that are in a cluster. \n",
    "\n",
    "* *robustness to noise and outliers*: very good usually, since we actually improved on DBSCAN\n",
    "\n",
    "* *Occam's razor*: rather intuitive parameter that does, however, depend on the dataset\n",
    "\n",
    "* *stability*: stable across runs and parameters\n",
    "\n",
    "* *performance with high-dimensional data*: ok-ish, but not as good as k-means\n",
    "\n",
    "* *performance scalability*: ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "plot_clusters2D(data2D, hdbscan.HDBSCAN, (), {'min_cluster_size':15})\n",
    "# same parameters for IRIS\n",
    "plot_clustersIRIS(iris, hdbscan.HDBSCAN, (), {'min_cluster_size':15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
