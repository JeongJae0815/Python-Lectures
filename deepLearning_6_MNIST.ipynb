{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import pandas\n",
    "import sys\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import csv\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning of MNIST data using tensorflow\n",
    "\n",
    "We closely follow the tensorflow tutorial here, trying to train and optimize a neural network to recognize our handwritten digits.\n",
    "\n",
    "## Importing MNIST\n",
    "\n",
    "Tensorflow has its own version of the MNIST data, which we import like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training in an interactive session\n",
    "\n",
    "So far, we have created our graph and then called `sess.run()` to evaluate it. We can be a bit more flexible and run an interactive session that lets us change things under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the data\n",
    "\n",
    "We first need input and label data. These are tensorflow placeholders that will be populated with the actual data later on. \n",
    "\n",
    "Since MNIST images are 28x28 pixels, the resulting dimensionality is 784. And we have 10 digit labels, which makes the output a **one-hot encoded** vector with 10 dimensions. \n",
    "\n",
    "In order to initialize these properly, we also tell tensorflow the size of each of the inputs. This is not actually necessary, but will allow tensorflow later to debug dimension-mismatches (which happen a lot in neural nets!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# None tells tensorflow that that dimension can have any size\n",
    "x = tf.placeholder(tf.float32, shape=[None,784])\n",
    "y = tf.placeholder(tf.float32, shape=[None,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: A linear model with softmax loss\n",
    "\n",
    "We know that a Linear Ridge regression classifier can get us around 88% accuracy on the MNIST data.\n",
    "\n",
    "Let's use a similar model, but use the typical softmax loss, which is used to optimize classification problems.\n",
    "\n",
    "First the linear model with its parameters. Don't forget that we need to initialize variables before their first use!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# weights map all dimensions to ten classes\n",
    "w = tf.Variable(tf.zeros([784,10]))\n",
    "# biases get added\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# note how tensorflow expects the multiplication order\n",
    "ym = tf.matmul(x,w)+b\n",
    "# the output of course has to be label-dimension!\n",
    "print(ym.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the loss function. We will take the usual softmax cross-entropy function, that is implemented in tensorflow. \n",
    "\n",
    "The softmax is applied to the output prediction of the model ($ym$) and then summed across all classes ($\\sum_{c=1}^{10}$). The average of all sums is then our total loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=ym))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Let's do a minibatch gradient descent, where we feed in 100 examples and then train the parameters successively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = tf.train.GradientDescentOptimizer(0.5).minimize(l)\n",
    "for iter in np.arange(1000):\n",
    "    miniBatch = mnist.train.next_batch(100)\n",
    "    train.run(feed_dict={x: miniBatch[0], y: miniBatch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "We check how many labels are correct (result of that is boolean) and then cast that to a float variable (the result of that is numbers [0,1]) and then average this to get the overall accuracy.\n",
    "\n",
    "Note, that these are not executed yet but only define the computational operations in the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = tf.equal(tf.argmax(y,1),tf.argmax(ym,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Linear accuracy:',accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to regression loss\n",
    "\n",
    "Let's use our \"normal\" L2 regression loss (sometimes called Euclidean loss). This is going to be very ugly, since we will require the classifier to produce weights to model one-hot vectors as closely as possible!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standard Euclidean loss - note, this is a bad idea\n",
    "l = tf.reduce_mean(tf.nn.l2_loss(t=y-ym))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# we need to change the learning rate in order to get somewhere with this, \n",
    "# since we have not normalized our data\n",
    "train = tf.train.GradientDescentOptimizer(0.000005).minimize(l)\n",
    "for it in np.arange(10000):\n",
    "    miniBatch = mnist.train.next_batch(100)\n",
    "    train.run(feed_dict={x: miniBatch[0], y: miniBatch[1]})\n",
    "    if (it%1000==0):\n",
    "        print(it,sess.run(l,feed_dict={x: miniBatch[0], y: miniBatch[1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Linear accuracy:',accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going deep!\n",
    "\n",
    "Now, let's train a proper network architecture!\n",
    "\n",
    "We will use some convnets with ReLU activation functions and softmax cross-entropy loss at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1 - CONV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "# layer 1 weights and biases\n",
    "# we use 5x5 filters, the input dimensionality is \"1\" and 32 different filters\n",
    "# the variable is initialized from normally distributed numbers with\n",
    "# the given standard deviation\n",
    "w_conv1 = tf.Variable(tf.truncated_normal([5,5,1,32],stddev=0.1))\n",
    "# we should not forget the biases!\n",
    "b_conv1 = tf.Variable(tf.constant(0.1,shape=[32]))\n",
    "# layer 1 input to convolution needs to be reshaped to a 4D tensor\n",
    "# note that this shape is required by tf.nn.conv2d!!!\n",
    "x_tensor = tf.reshape(x,[-1,28,28,1])\n",
    "# layer 1 output is convolution, followed by RELU\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(x_tensor,w_conv1,strides=[1,1,1,1],padding='SAME')+b_conv1)\n",
    "# layer 1 pooling of convolution layer, we use standard max-2 pooling\n",
    "h_pool1 = tf.nn.max_pool(h_conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "print(h_pool1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2 - CONV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# layer 2 weights and biases\n",
    "# we use 5x5 filters, the input dimensionality is 32 and we want 64 filters here\n",
    "w_conv2 = tf.Variable(tf.truncated_normal([5,5,32,64],stddev=0.1))\n",
    "# and again the biases\n",
    "b_conv2 = tf.Variable(tf.constant(0.1,shape=[64]))\n",
    "# layer 2 output is convolution, followed by RELU\n",
    "# note that the input consists of course of layer1 output, which is a 4D tensor\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1,w_conv2,strides=[1,1,1,1],padding='SAME')+b_conv2)\n",
    "# layer 2 pooling of the convolution layer with max-2 pooling\n",
    "h_pool2 = tf.nn.max_pool(h_conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "print(h_pool2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3 - FC\n",
    "\n",
    "Now we put on the final, fully-connected layer, which takes the 7x7x64-dimensional tensor output from the second CONV layer and implements a standard ReLU layer activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fully connected layer\n",
    "w_fc1 = tf.Variable(tf.truncated_normal([7*7*64,1024],stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1,shape=[1024]))\n",
    "\n",
    "# we need to reshape the input to 7*7*64 (i.e., flatten it)\n",
    "h_pool2_flattened = tf.reshape(h_pool2,[-1, 7*7*64])\n",
    "\n",
    "# this will result in a flat, 1024 dimensional output\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flattened,w_fc1)+b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trick 1 - Dropout\n",
    "\n",
    "CNNs with this many parameters can be prone to overfitting. One way to combat this is to use dropouts during training. This means that each weight is kept only with a certain probability during training.\n",
    "\n",
    "We can add dropouts on the fully-connected layer as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropout layer on the previous FC layer\n",
    "keep_connection_probability = tf.placeholder(tf.float32)\n",
    "h_fc1_dropout = tf.nn.dropout(h_fc1,keep_connection_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-out and softmax loss\n",
    "Now we add the final layer that takes the 1024 values from the FC layer and maps them to a 10-dimensional vector on which we finally can evaluate our loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read-out layer that will feed into softmax cross-entropy loss\n",
    "w_fc2 = tf.Variable(tf.truncated_normal([1024,10],stddev=0.1))\n",
    "b_fc2 = tf.Variable(tf.constant(0.1,shape=[10]))\n",
    "\n",
    "# the final output as activations of the final, fully-connected layer\n",
    "y_fc2 = tf.matmul(h_fc1_dropout,w_fc2)+b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loss \"layer\", which is the standard softmax cross-entropy\n",
    "l = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_fc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "Let's use a more sophisticated training scheme - the Adam optimizer. This automatically adjusts the learning rate (initialized to be 0.0001 here) over the course of the training based on the changes in gradients and the changes in the loss function. It is one of the most-used optimization algorithms for these kinds of networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select Adam and initialize learning rate\n",
    "train = tf.train.AdamOptimizer(1e-4).minimize(l)\n",
    "# evaluate number of correctly-predicted items and accuracy as before\n",
    "correct = tf.equal(tf.argmax(y_fc2,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "# init\n",
    "sess.run(tf.global_variables_initializer())\n",
    "batch = 0\n",
    "train_accuracy=list()\n",
    "# run this 4000 times\n",
    "for i in range(4000):\n",
    "    # get next batch\n",
    "    miniBatch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        # evaluate accuracy with current mini batch\n",
    "        train_accuracy.append(accuracy.eval(feed_dict={\n",
    "            x:miniBatch[0], y: miniBatch[1], keep_connection_probability: 1.0}))\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy[-1]))\n",
    "        batch = batch + 1\n",
    "    # for each batch, run one training step with the data and the dropout probability\n",
    "    train.run(feed_dict={x: miniBatch[0], y: miniBatch[1], keep_connection_probability: 0.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('CNN test accuracy:',accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_connection_probability: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from showTensorflowGraph import show_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait - what? Why does this look so chaotic? Where are the layers? Where is my beautiful \n",
    "\n",
    "CONV-RELU-CONV-RELU-FC-DROPOUT-LOGIT-SOFTMAX \n",
    "\n",
    "architecture?\n",
    "\n",
    "The reason we got this \"chaotic\" graph is that we used the lower-level APIs of tensorflow. If you want a prettier graph that hides some of the complexity of the updates and gradients, etc., you should use the `layers` functionality of the tensorflow API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
